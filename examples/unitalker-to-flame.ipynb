{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adaa171c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peizhi/miniconda3/envs/unitalker/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight path:  ./pretrained_models/UniTalker-B-D0-D7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peizhi/miniconda3/envs/unitalker/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# WORKING_DIR = './' # change the working directory to the project's absolute path\n",
    "# os.chdir(WORKING_DIR)\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "import pickle\n",
    "\n",
    "from dataset.dataset_config import dataset_config\n",
    "from loss.loss import UniTalkerLoss\n",
    "from models.unitalker import UniTalker\n",
    "from utils.utils import get_parser, get_template_verts, get_audio_encoder_dim\n",
    "from utils import config\n",
    "\n",
    "def get_all_audios(audio_root:str):\n",
    "    wav_f_names = []\n",
    "    for r, _, f in os.walk(audio_root):\n",
    "        for wav in f:\n",
    "            if wav.endswith('.wav') or wav.endswith('.mp3'):\n",
    "                relative_path = os.path.join(r, wav)\n",
    "                relative_path = os.path.relpath(relative_path,\n",
    "                                                audio_root)\n",
    "                wav_f_names.append(relative_path)\n",
    "    wav_f_names = sorted(wav_f_names)\n",
    "    return wav_f_names\n",
    "\n",
    "def split_long_audio(\n",
    "    audio: np.ndarray,\n",
    "    processor:Wav2Vec2FeatureExtractor\n",
    "):\n",
    "    # audio = audio.squeeze(0)\n",
    "    a, b = 25, 5\n",
    "    sr = 16000 \n",
    "    total_length = len(audio) /sr\n",
    "    reps = max(0, int(np.ceil((total_length - a) / (a - b)))) + 1\n",
    "    in_audio_split_list = []\n",
    "    start, end = 0, int(a * sr)\n",
    "    step = int((a - b) * sr)\n",
    "    for i in range(reps):\n",
    "        audio_split = audio[start:end]\n",
    "        audio_split = np.squeeze(\n",
    "            processor(audio_split, sampling_rate=sr).input_values)\n",
    "        in_audio_split_list.append(audio_split)\n",
    "        start += step\n",
    "        end += step\n",
    "    return in_audio_split_list\n",
    "\n",
    "def merge_out_list(out_list: list, fps:int):\n",
    "    if len(out_list) == 1:\n",
    "        return out_list[0]\n",
    "    a, b = 25, 5\n",
    "    left_weight = np.linspace(1, 0, b * fps)[:, np.newaxis]\n",
    "    right_weight = 1 - left_weight\n",
    "    a = a * fps \n",
    "    b = b * fps \n",
    "    offset = a - b\n",
    "\n",
    "    out_length = len(out_list[-1]) + offset * (len(out_list) - 1)\n",
    "    merged_out = np.empty((out_length, out_list[-1].shape[-1]),\n",
    "                            dtype=out_list[-1].dtype)\n",
    "    merged_out[:a] = out_list[0]\n",
    "    for out_piece in out_list[1:]:\n",
    "        merged_out[a - b:a] = left_weight * merged_out[\n",
    "            a - b:a] + right_weight * out_piece[:b]\n",
    "        merged_out[a:a + offset] = out_piece[b:]\n",
    "        a += offset\n",
    "    return merged_out\n",
    "\n",
    "condition_id_config = {\n",
    "    'D0': 3,\n",
    "    'D1': 3,\n",
    "    'D2': 0,\n",
    "    'D3': 0,\n",
    "    'D4': 0,\n",
    "    'D5': 0,\n",
    "    'D6': 4,\n",
    "    'D7': 0,\n",
    "}\n",
    "template_id_config = {\n",
    "    'D0': 3,\n",
    "    'D1': 3,\n",
    "    'D2': 0,\n",
    "    'D3': 0,\n",
    "    'D4': 0,\n",
    "    'D5': 0,\n",
    "    'D6': 4,\n",
    "    'D7': 0,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "cfg = config.load_cfg_from_cfg_file('./config/unitalker.yaml')\n",
    "# if args.opts is not None:\n",
    "#     cfg = config.merge_cfg_from_list(cfg, args.opts)\n",
    "cfg.condition_id = 'common'\n",
    "cfg.dataset = cfg.dataset.split(',')\n",
    "cfg.demo_dataset = cfg.demo_dataset.split(',')\n",
    "\n",
    "print('Weight path: ', cfg.weight_path)\n",
    "\n",
    "checkpoint = torch.load(cfg.weight_path, map_location='cpu')\n",
    "cfg.identity_num = len(checkpoint['decoder.learnable_style_emb.weight'])\n",
    "\n",
    "dataset_name = 'D1' # FLAME\n",
    "\n",
    "start_idx = 0\n",
    "annot_type = dataset_config[dataset_name]['annot_type']\n",
    "id_num = dataset_config[dataset_name]['subjects']\n",
    "end_idx = start_idx + id_num\n",
    "local_condition_idx = condition_id_config[dataset_name]\n",
    "template_idx = template_id_config[dataset_name]\n",
    "template = get_template_verts(cfg.data_root, dataset_name, template_idx)\n",
    "template_id_config[dataset_name] = torch.Tensor(template.reshape(1, -1))\n",
    "if cfg.condition_id == 'each':\n",
    "    condition_id_config[dataset_name] = torch.tensor(\n",
    "        start_idx + local_condition_idx).reshape(1)\n",
    "elif cfg.condition_id == 'common':\n",
    "    condition_id_config[dataset_name] = torch.tensor(cfg.identity_num - 1).reshape(1)\n",
    "else:\n",
    "    try:\n",
    "        condition_id = int(cfg.condition_id)\n",
    "        condition_id_config[dataset_name] = torch.tensor(\n",
    "            condition_id).reshape(1)\n",
    "    except ValueError:\n",
    "        assert cfg.condition_id in dataset_config.keys()\n",
    "        condition_id_config[dataset_name] = torch.tensor(\n",
    "            start_idx + local_condition_idx).reshape(1)\n",
    "start_idx = end_idx\n",
    "\n",
    "\n",
    "cfg.audio_encoder_feature_dim = get_audio_encoder_dim(cfg.audio_encoder_repo)\n",
    "\n",
    "model = UniTalker(cfg)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(cfg.audio_encoder_repo,)\n",
    "loss_module = UniTalkerLoss(cfg).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "725cebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "wav_path = './test_audios/can_you_feel_the_love_tonight_clip.wav'\n",
    "save_path = './can_you_feel_the_love_tonight_clip.npy'\n",
    "fps = 30\n",
    "\n",
    "\n",
    "# run unitalker model\n",
    "with torch.no_grad():\n",
    "    annot_type = 'flame_params_from_dadhead'\n",
    "    audio_data, sr = librosa.load(wav_path, sr=16000)\n",
    "    audio_data = np.squeeze(processor(audio_data, sampling_rate=sr).input_values)\n",
    "\n",
    "    audio_data_splits = split_long_audio(audio_data, processor)\n",
    "\n",
    "    template = template_id_config[dataset_name].cuda()\n",
    "    scale = dataset_config[dataset_name]['scale']\n",
    "    template = scale * template\n",
    "    condition_id = condition_id_config[dataset_name].cuda()\n",
    "\n",
    "    out_list = []\n",
    "    for audio_data in audio_data_splits:\n",
    "        audio_data = torch.Tensor(audio_data[None]).cuda()\n",
    "\n",
    "        frame_num = round(audio_data.shape[-1] / 16000 * fps)\n",
    "        hidden_states = model.audio_encoder(\n",
    "            audio_data, frame_num=frame_num, interpolate_pos=model.interpolate_pos)\n",
    "        hidden_states = hidden_states.last_hidden_state\n",
    "        decoder_out = model.decoder(hidden_states, condition_id, frame_num)\n",
    "        out_motion = model.out_head_dict[annot_type](decoder_out)[0]\n",
    "        out_list.append(out_motion.detach().cpu().numpy())\n",
    "\n",
    "    #out = np.concatenate(out_list, axis=0)\n",
    "    out = merge_out_list(out_list, fps=fps)\n",
    "\n",
    "\n",
    "out_dict = {\n",
    "    'exp': out[:, 300:400], # extract expression coefficients\n",
    "    'jaw': out[:, 400:403], # extract jaw pose\n",
    "    'fps': fps\n",
    "}\n",
    "\n",
    "# save the sequence of driving signals (later used in Gaussian Dejavu)\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(out_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e991646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unitalker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
